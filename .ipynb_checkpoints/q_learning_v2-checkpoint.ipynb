{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "updated from: http://mnemstudio.org/path-finding-q-learning-tutorial.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward matrix shape:  (6, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ -1,  -1,  -1,  -1,   0,  -1],\n",
       "        [ -1,  -1,  -1,   0,  -1, 100],\n",
       "        [ -1,  -1,  -1,   0,  -1,  -1],\n",
       "        [ -1,   0,   0,  -1,   0,  -1],\n",
       "        [  0,  -1,  -1,   0,  -1, 100],\n",
       "        [ -1,   0,  -1,  -1,   0, 100]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_matrix = np.matrix('''-1 -1 -1 -1 0 -1;\n",
    "                        -1 -1 -1 0 -1 100; \n",
    "                        -1 -1 -1 0 -1 -1; \n",
    "                        -1 0 0 -1 0 -1; \n",
    "                        0 -1 -1 0 -1 100; \n",
    "                        -1 0 -1 -1 0 100''')\n",
    "\n",
    "print('reward matrix shape: ', r_matrix.shape)\n",
    "r_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q matrix shape:  (6, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_matrix = np.zeros((6,6), dtype=np.int)\n",
    "\n",
    "print('q matrix shape: ', q_matrix.shape)\n",
    "q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "initial_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices:  [[0 3]\n",
      " [0 5]]\n"
     ]
    }
   ],
   "source": [
    "def positiveRewards(state):\n",
    "    '''\n",
    "    - given a state we should spit out the states not -1\n",
    "    - we should also keep a note of the indices of above numbers\n",
    "    - after getting the indices we \n",
    "    '''\n",
    "    # for state=1 ==> matrix([[ -1,  -1,  -1,   0,  -1, 100]])\n",
    "    # output dimension of this is 2\n",
    "    matrixListForChosenState = r_matrix[state]\n",
    "    \n",
    "    # for the above matrix output is\n",
    "    # array([[0, 3],\n",
    "    #       [0, 5]], dtype=int32), you can see 3rd and 5th element of 0th row are the >-1 values\n",
    "    indices = np.argwhere(matrixListForChosenState > -1)\n",
    "\n",
    "    return indices\n",
    "\n",
    "indices = positiveRewards(initial_state)\n",
    "print('indices: ', indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionChosen = indices[np.random.choice(range(len(indices)))][1]\n",
    "actionChosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = r_matrix[initial_state, actionChosen]\n",
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 4],\n",
       "       [0, 5]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrayOfNextStates = positiveRewards(actionChosen)\n",
    "arrayOfNextStates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qValuesForOtherState(actionChosen, arrayOfNextStates):\n",
    "    qValues = []\n",
    "    for element in arrayOfNextStates[:,1]:\n",
    "        qValues.append(q_matrix[actionChosen, element])\n",
    "        \n",
    "    return np.max(qValues)\n",
    "\n",
    "qMax = qValuesForOtherState(actionChosen, arrayOfNextStates)\n",
    "qMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run the next Line or section 2 will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 100],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,  80,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_matrix[initial_state, actionChosen] = reward + gamma*qMax\n",
    "q_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This section is a condensed version of procedure above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_matrix = np.matrix('''-1 -1 -1 -1 0 -1;\n",
    "                        -1 -1 -1 0 -1 100; \n",
    "                        -1 -1 -1 0 -1 -1; \n",
    "                        -1 0 0 -1 0 -1; \n",
    "                        0 -1 -1 0 -1 100; \n",
    "                        -1 0 -1 -1 0 100''')\n",
    "\n",
    "q_matrix = np.zeros((6,6), dtype=np.int)\n",
    "\n",
    "def positiveRewards(state):\n",
    "    '''\n",
    "    - given a state we should spit out the states not -1\n",
    "    - we should also keep a note of the indices of above numbers\n",
    "    - after getting the indices we \n",
    "    '''\n",
    "    # for state=1 ==> matrix([[ -1,  -1,  -1,   0,  -1, 100]])\n",
    "    # output dimension of this is 2\n",
    "    matrixListForChosenState = r_matrix[state]\n",
    "    \n",
    "    # for the above matrix output is\n",
    "    # array([[0, 3],\n",
    "    #       [0, 5]], dtype=int32), you can see 3rd and 5th element of 0th row are the >-1 values\n",
    "    indices = np.argwhere(matrixListForChosenState > -1)\n",
    "\n",
    "    return indices\n",
    "\n",
    "def qValuesForOtherState(actionChosen, arrayOfNextStates):\n",
    "    qValues = []\n",
    "    for element in arrayOfNextStates[:,1]:\n",
    "        qValues.append(q_matrix[actionChosen, element])\n",
    "        \n",
    "    return np.max(qValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_update(initial_state, q_matrix):\n",
    "    indices = positiveRewards(initial_state)\n",
    "    actionChosen = indices[np.random.choice(range(len(indices)))][1]\n",
    "    reward = r_matrix[initial_state, actionChosen]\n",
    "    arrayOfNextStates = positiveRewards(actionChosen)\n",
    "    q_matrix[initial_state, actionChosen] = reward + gamma * qValuesForOtherState(actionChosen, arrayOfNextStates)\n",
    "    return indices, actionChosen, reward, arrayOfNextStates, q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "initial_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def episode(initial_state):\n",
    "    while initial_state != 5:    \n",
    "        _, actionChosen, _, _, _ = q_update(initial_state, q_matrix)\n",
    "        initial_state = actionChosen\n",
    "    return q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    for _ in range(500):\n",
    "        episode(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,  80,   0],\n",
       "       [  0,   0,   0,  64,   0, 100],\n",
       "       [  0,   0,   0,  64,   0,   0],\n",
       "       [  0,  80,  51,   0,  80,   0],\n",
       "       [ 64,   0,   0,  64,   0, 100],\n",
       "       [  0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 1, 5]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pathToReward(start_index, q_matrix_input):\n",
    "    path = [start_index]\n",
    "    while start_index != 5:\n",
    "        arg = np.argwhere(q_matrix_input[start_index] == np.max(q_matrix_input[start_index]))\n",
    "        new_index = arg.tolist()[0][0]\n",
    "        path.append(new_index)\n",
    "        start_index = new_index\n",
    "    return path\n",
    "    \n",
    "pathToReward(2, q_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section is for testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices:  [[0 1]\n",
      " [0 2]\n",
      " [0 4]]\n",
      "action chosen:  1\n",
      "reward:  0\n",
      "array of indices:  [[0 3]\n",
      " [0 5]]\n",
      "q_matrix:  [[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# indices, actionChosen, reward, arrayOfNextStates, q_matrix = q_update(initial_state, q_matrix)\n",
    "\n",
    "# print('indices: ', indices)\n",
    "# print('action chosen: ', actionChosen)\n",
    "# print('reward: ', reward)\n",
    "# print('array of indices: ', arrayOfNextStates)\n",
    "# print('q_matrix: ', q_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3\n",
    "#### Here is defining a new class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon=0.1, alpha=0.2, gamma=0.9):\n",
    "        self.q = {}\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "        # return self.q.get((state, action), 1.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        oldv = self.q.get((state, action), None)\n",
    "        if oldv is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldv + self.alpha * (value - oldv)\n",
    "\n",
    "    def chooseAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            action = random.choice(self.actions)\n",
    "        else:\n",
    "            q = [self.getQ(state, a) for a in self.actions]\n",
    "            maxQ = max(q)\n",
    "            count = q.count(maxQ)\n",
    "            if count > 1:\n",
    "                best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "                i = random.choice(best)\n",
    "            else:\n",
    "                i = q.index(maxQ)\n",
    "\n",
    "            action = self.actions[i]\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
    "\n",
    "import math\n",
    "def ff(f,n):\n",
    "    fs = \"{:f}\".format(f)\n",
    "    if len(fs) < n:\n",
    "        return (\"{:\"+n+\"s}\").format(fs)\n",
    "    else:\n",
    "        return fs[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# borrowing from the q learningstudywolf.wordpress.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the gridworld we have only 4 possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = ['up','down','left','right']\n",
    "actionNum = [index for index, _ in enumerate(actions)]\n",
    "\n",
    "# actionNum encodes: up, down, left and right as 0, 1, 2 and 3 respectively.\n",
    "actionNum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the gridworld. And let it be a 4 by 4 pixaleted world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3),\n",
       " (3, 0),\n",
       " (3, 1),\n",
       " (3, 2),\n",
       " (3, 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our states will be 16 set of tuples \n",
    "states = []\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        states.append((i,j))\n",
    "        \n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (3, 3) (2, 3) 13\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "statesAvailable = states[:]\n",
    "\n",
    "# wallState = states[np.random.randint(0,len(statesAvailable))]\n",
    "# posState = states[np.random.randint(0,len(statesAvailable))]\n",
    "# negState = states[np.random.randint(0,len(statesAvailable))]\n",
    "\n",
    "wallState = (2,1)\n",
    "posState = (3,3)\n",
    "negState = (2,3)\n",
    "\n",
    "statesAvailable.pop(statesAvailable.index(wallState))\n",
    "statesAvailable.pop(statesAvailable.index(posState))\n",
    "statesAvailable.pop(statesAvailable.index(negState))\n",
    "\n",
    "print(wallState, posState, negState, len(statesAvailable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0],\n",
       "       [   0,   -1,    0, -100],\n",
       "       [   0,    0,    0,  100]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have wall, pos and negative states defined\n",
    "# we are going to create and update the reward matrix\n",
    "r_matrix = np.zeros((4,4), dtype=np.int)\n",
    "r_matrix[wallState] = -1\n",
    "r_matrix[posState] = 100\n",
    "r_matrix[negState] = -100\n",
    "\n",
    "r_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now defining the car state\n",
    "carState = (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, -1, 0, -1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# given coordinate and grid size it will tell if [up,down,left,right]\n",
    "# is available\n",
    "def actionsAvailable(state, row_max, col_max):\n",
    "    row, col = state\n",
    "    directions = [-1,-1,-1,-1]\n",
    "    if row < row_max: directions[1] = 0\n",
    "    if row > 0: directions[0] = 0\n",
    "    if col < col_max: directions[3] = 0\n",
    "    if col > 0: directions[2] = 0\n",
    "    return directions\n",
    "\n",
    "actionsAvailable((3,3),3,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the equation that will be used:\n",
    "\n",
    "Q(S_t,A_t) = Q(S_t,A_t) + alpha X [R_t+1 + gamma X max{Q(S_t+1,A_t+1)}-Q(S_t,A_t)]\n",
    "\n",
    "- some things to be congnicent off:\n",
    "1. R_t+1 is the reward for the state that you are going to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_matrix = np.zeros((len(states),len(actionNum)), dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1,  0, -1,  0],\n",
       "       [-1,  0,  0,  0],\n",
       "       [-1,  0,  0,  0],\n",
       "       [-1,  0,  0, -1],\n",
       "       [ 0,  0, -1,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1],\n",
       "       [ 0,  0, -1,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0],\n",
       "       [ 0,  0,  0, -1],\n",
       "       [ 0, -1, -1,  0],\n",
       "       [ 0, -1,  0,  0],\n",
       "       [ 0, -1,  0,  0],\n",
       "       [ 0, -1,  0, -1]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start state:  (3, 2)\n",
      "action taken:  2\n",
      "Action takes us to this state:  (3, 1)\n",
      "Reward at state we are in:  0\n",
      "Q matrix:  [[-1  0 -1  0]\n",
      " [-1  0  0  0]\n",
      " [-1  0  0  0]\n",
      " [-1  0  0 -1]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0 -1]\n",
      " [ 0  0 -1  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0 -1]\n",
      " [ 0 -1 -1  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0 -1  0  0]\n",
      " [ 0 -1  0 -1]]\n"
     ]
    }
   ],
   "source": [
    "row_max = 3\n",
    "\n",
    "# we are going to update q_matrix by putting -1 where we can't go\n",
    "for state in states:\n",
    "    qRowNum = int((16/(3 + 1))*state[0] + state[1])\n",
    "    q_matrix[qRowNum] = actionsAvailable(state,3,3)\n",
    "    \n",
    "def choose_action(state, row_max, col_max):\n",
    "    '''\n",
    "    1. given state get its row number [qRowNum]\n",
    "    2. for qRowNum get all column items\n",
    "    3. find max value amongnst all column(action) elements [qMax]\n",
    "    4. get indices in from q where column elements == qMax [indices]\n",
    "    5. random choice on any element in 'indices'\n",
    "    '''\n",
    "    qRowNum = int((16/(row_max + 1))*state[0] + state[1])\n",
    "    q = list(q_matrix[qRowNum])\n",
    "    qMax = np.max(q)\n",
    "    \n",
    "    indices = np.argwhere(q == qMax)\n",
    "    chooseActionIndex = np.random.choice(indices.flatten('F'))\n",
    "    return chooseActionIndex\n",
    "\n",
    "# now that we know the action to take (from choose_action)\n",
    "# get state where action leads us\n",
    "def stateTo(state, actionTaken):\n",
    "    row, col = state\n",
    "    if actionTaken == 0: row -= 1\n",
    "    if actionTaken == 1: row += 1\n",
    "    if actionTaken == 2: col -= 1\n",
    "    if actionTaken == 3: col += 1\n",
    "        \n",
    "    return row,col\n",
    "\n",
    "# Now get reward in this state\n",
    "def getReward(state):\n",
    "    return r_matrix[state[0], state[1]]\n",
    "\n",
    "def updateQMatrix(state, actionChosen, row_max, reward):\n",
    "    qRowNum = int((16/(row_max + 1))*state[0] + state[1])\n",
    "    q_matrix[qRowNum, actionChosen] = reward\n",
    "    return q_matrix\n",
    "    \n",
    "\n",
    "# !!!!! STARTING HERE !!!!\n",
    "state = (3,2)\n",
    "actionChosen = choose_action(state,3,3)\n",
    "stateSentTo = stateTo(state, actionChosen)\n",
    "rewardInNewState = getReward(stateSentTo)\n",
    "updateQMatrix(state, actionChosen, row_max, rewardInNewState)\n",
    "\n",
    "print('start state: ', state)\n",
    "print('action taken: ', actionChosen)\n",
    "print('Action takes us to this state: ', stateSentTo)\n",
    "print('Reward at state we are in: ', rewardInNewState)\n",
    "print('Q matrix: ', q_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
